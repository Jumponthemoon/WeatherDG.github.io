
# WeatherDG: 基于大型语言模型辅助的渐进式天气效果生成，可无限扩充自动驾驶天气数据集

## 摘要
在这项工作中，我们提出了一种新颖的方法，名为WeatherDG，它可以通过两个基础模型的合作，即Stable Diffusion（SD）和大型语言模型（LLM），生成大量逼真、多样化的天气驾驶场景图像。具体来说，我们首先使用源数据对SD进行微调，使生成的样本内容和布局与真实驾驶场景保持一致。然后，我们基于LLM提出了一种渐进化的提示生成方法，该方法可以丰富场景描述，帮助SD自动生成更多样化、详细的图像。此外，我们引入了一种平衡生成策略，鼓励SD在各种天气条件下生成高质量的“尾类”对象，例如骑车人和摩托车。这种与分割模型无关的方法通过使用生成的合成数据来进一步适应现有模型，从而提高它们的泛化能力。实验结果表明，我们的方法可以显著提高不同最先进模型在目标领域上的分割性能。特别是在“Cityscapes到ACDC”的实验设置中，我们的方法使基线模型HRDA的mIoU提高了13.9%。更多结果请参见项目页面：weatherDG.github.io。

## 1. 引言
语义分割是自动驾驶的基本任务。尽管在这一领域取得了显著的成就，现有模型在部署到未知领域时仍面临严重挑战，这源于众所周知的领域偏移问题。此外，当未见领域具有恶劣天气条件时，这一问题会更加严重，如雾天、雨天、雪天和夜晚场景。

一种解决上述问题的简单方法是收集更多样化的训练数据。然而，标注分割任务需要大量时间，因为我们需要为图像中的每个像素进行标注。因此，领域泛化在解决领域偏移问题中变得流行，其目标是在仅使用给定源数据的情况下训练一个能够泛化到未见领域的模型。现有的领域泛化方法一般可以分为两类：归一化方法和数据增强方法。本文我们主要关注后者，因为它更灵活且易于与前者技术集成。以往方法通常使用模拟器或图像转换模型生成新样本，虽然有效，但在生成恶劣天气条件下的样本时，仍存在多样性和真实性问题（如图2所示）。近年来，Stable Diffusion（SD）展示了生成逼真、多样化高质量图像的强大能力，激发了我们利用SD来解决以往数据增强方法在领域泛化中的不足。然而，直接将SD应用于我们的任务会产生一个关键问题：生成的样式和布局与驾驶场景样本差异较大（见图2b）。由于SD的训练数据包含各种类型的图像，而不是专门针对驾驶场景，导致无法生成具有驾驶屏幕特征的样本，缺乏细致和明确的指导。

## 2. 相关工作

### 领域泛化语义分割（DGSS）
DGSS旨在训练深度神经网络，使其在多个未见领域的语义分割任务中表现良好。现有的DGSS方法通过归一化或数据增强来解决领域差异问题。归一化方法通过对源特征的均值和标准差进行归一化训练，或通过白化这些特征的协方差来解决问题。基于数据增强的方法则将源图像转化为随机风格化的版本，以引导模型捕捉领域不变的形状特征，因为纹理提示被随机风格替代。

### 无监督领域自适应（UDA）
UDA旨在提升模型在领域特定数据上的表现，而无需标注的示例。现有的UDA技术可以分为三类：差异最小化、对抗性训练和自训练。最近，DATUM [27] 提出了一种单次域自适应方法，该方法使用目标域中的单个图像生成数据集，并结合无监督域自适应训练方法来弥合模拟到现实的差距。此外，PODA [28] 利用了CLIP模型的能力，通过提示实现了零样本域自适应。

### 基于文本的图像生成
当前的文本生成图像任务主要由基于扩散模型和大语言模型（LLM）的方法驱动。扩散模型在生成逼真图像方面取得了突破，促使研究人员探索其在丰富源域数据集和改进语义分割中的应用。例如，DIDEX [8] 利用 ControlNet [30] 将合成图像转换为现实世界风格。然而，这种方法通常缺乏真实感，并重复训练数据的空间布局，限制了多样性。

另一方面，大型语言模型也发挥着关键作用。CuPL [31] 利用 GPT-3 [32] 生成文本描述，增强零样本图像分类。CLOUDS [9] 使用 Llama [33] 创建用于扩散模型的提示。然而，它们未能充分考虑因天气和光照条件变化带来的复杂性。相比之下，我们的方法采用了一系列作为代理的LLM，不仅能够为复杂的现实场景生成详细描述，还实施了定制的生成策略。这样保证了生成的图像既多样又真实，且能够解决在复杂条件下的类别不平衡问题。

## 3. 提出的方法

WeatherDG旨在生成适用于特定天气的自动驾驶场景图像，以增强恶劣条件下的语义分割性能。我们首先通过微调扩散模型来适应源领域的场景先验，确保生成的图像符合驾驶场景。接着，我们采用程序化的提示生成方法，创建详细的提示，使扩散模型能够生成逼真且多样化的天气和光照效果。最后，我们利用UDA训练方法来使用生成的图像进行语义分割模型的训练。

### 3.1 SD微调
首先，使用源数据...
